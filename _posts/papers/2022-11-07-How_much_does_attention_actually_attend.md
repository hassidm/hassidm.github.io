---
title: "How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers"
authors: <b>Michael Hassid</b>, Hao Peng, Daniel Rotem, Jungo Kasai, Ivan Montero, Noah A Smith, Roy Schwartz
venue: Findings of EMNLP 2022
base: eco-sem
bib: NONE
bib-ext: NONE
pdf: NONE
pdf-ext: https://aclanthology.org/2022.findings-emnlp.101/
data: NONE
talk: NONE
code: https://github.com/schwartz-lab-NLP/papa
layout: post
poster: NONE
page: NONE
arxiv: https://arxiv.org/abs/2211.03495
slides: NONE
date: 7-06-2022
---
